Using cpu device
Wrapping the env with a `Monitor` wrapper
Wrapping the env in a DummyVecEnv.
Logging to ./lego_tensorboard/nmax 14 ntensor 6 (normalized) None_1
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 2.22     |
|    ep_rew_mean     | -8.29    |
| time/              |          |
|    fps             | 7        |
|    iterations      | 1        |
|    time_elapsed    | 12       |
|    total_timesteps | 100      |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.06        |
|    ep_rew_mean          | -8.29       |
| time/                   |             |
|    fps                  | 7           |
|    iterations           | 2           |
|    time_elapsed         | 25          |
|    total_timesteps      | 200         |
| train/                  |             |
|    approx_kl            | 0.024169896 |
|    clip_fraction        | 0.138       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | -0.00257    |
|    learning_rate        | 0.0003      |
|    loss                 | 2.57        |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0542     |
|    value_loss           | 17.7        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.49        |
|    ep_rew_mean          | -8.28       |
| time/                   |             |
|    fps                  | 7           |
|    iterations           | 3           |
|    time_elapsed         | 40          |
|    total_timesteps      | 300         |
| train/                  |             |
|    approx_kl            | 0.021899257 |
|    clip_fraction        | 0.11        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.96       |
|    explained_variance   | -0.00103    |
|    learning_rate        | 0.0003      |
|    loss                 | 1.25        |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0485     |
|    value_loss           | 3.89        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 3.84        |
|    ep_rew_mean          | -8.26       |
| time/                   |             |
|    fps                  | 7           |
|    iterations           | 4           |
|    time_elapsed         | 53          |
|    total_timesteps      | 400         |
| train/                  |             |
|    approx_kl            | 0.021546198 |
|    clip_fraction        | 0.143       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.89       |
|    explained_variance   | -2.47e-05   |
|    learning_rate        | 0.0003      |
|    loss                 | 0.545       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0377     |
|    value_loss           | 2.35        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 4.74        |
|    ep_rew_mean          | -8.24       |
| time/                   |             |
|    fps                  | 7           |
|    iterations           | 5           |
|    time_elapsed         | 67          |
|    total_timesteps      | 500         |
| train/                  |             |
|    approx_kl            | 0.028513808 |
|    clip_fraction        | 0.176       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.99       |
|    explained_variance   | 0.000435    |
|    learning_rate        | 0.0003      |
|    loss                 | 0.246       |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0516     |
|    value_loss           | 1.01        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 5.55        |
|    ep_rew_mean          | -8.19       |
| time/                   |             |
|    fps                  | 7           |
|    iterations           | 6           |
|    time_elapsed         | 81          |
|    total_timesteps      | 600         |
| train/                  |             |
|    approx_kl            | 0.047271777 |
|    clip_fraction        | 0.236       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.07       |
|    explained_variance   | -0.00539    |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0895     |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0605     |
|    value_loss           | 0.149       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 6.32        |
|    ep_rew_mean          | -8.16       |
| time/                   |             |
|    fps                  | 7           |
|    iterations           | 7           |
|    time_elapsed         | 93          |
|    total_timesteps      | 700         |
| train/                  |             |
|    approx_kl            | 0.052912302 |
|    clip_fraction        | 0.3         |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.39       |
|    explained_variance   | -0.00253    |
|    learning_rate        | 0.0003      |
|    loss                 | -0.109      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0678     |
|    value_loss           | 0.032       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 7.1         |
|    ep_rew_mean          | -8.13       |
| time/                   |             |
|    fps                  | 7           |
|    iterations           | 8           |
|    time_elapsed         | 107         |
|    total_timesteps      | 800         |
| train/                  |             |
|    approx_kl            | 0.034443837 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.09       |
|    explained_variance   | -0.00408    |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0522     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0519     |
|    value_loss           | 0.0932      |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 8.07        |
|    ep_rew_mean          | -8.1        |
| time/                   |             |
|    fps                  | 7           |
|    iterations           | 9           |
|    time_elapsed         | 119         |
|    total_timesteps      | 900         |
| train/                  |             |
|    approx_kl            | 0.028246477 |
|    clip_fraction        | 0.149       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.08       |
|    explained_variance   | -0.00076    |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00829    |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0333     |
|    value_loss           | 0.224       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 8.72        |
|    ep_rew_mean          | -8.09       |
| time/                   |             |
|    fps                  | 7           |
|    iterations           | 10          |
|    time_elapsed         | 133         |
|    total_timesteps      | 1000        |
| train/                  |             |
|    approx_kl            | 0.051761642 |
|    clip_fraction        | 0.28        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.99       |
|    explained_variance   | 0.0083      |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0986     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.065      |
|    value_loss           | 0.0158      |
-----------------------------------------
135.24125409126282
Wrote profile results to MPPO_script.py.lprof
Timer unit: 1e-06 s

Total time: 79.8318 s
File: MPPO_script.py
Function: is_valid at line 20

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    20                                           @profile
    21                                           def is_valid(state, action, env, num_max_actions=17, max_tensors=6, num_min_actions=0):
    22    637000    2237526.1      3.5      2.8      state = np.array(state)
    23                                               #env = Legoenv(max_tensors=max_tensors)
    24    637000    7557625.9     11.9      9.5      out_state = np.zeros_like(state)
    25    637000    5141685.2      8.1      6.4      terminal_state = np.zeros_like(state)
    26    637000     974119.4      1.5      1.2      terminal_state[-1] = 1
    27    637000     891362.6      1.4      1.1      if state[-1] == 1:
    28                                                   return False, terminal_state
    29                                           
    30                                               # make sure not too many actions are taken
    31    637000   11957966.0     18.8     15.0      if np.sum(state) >= num_max_actions:
    32     28028      39841.7      1.4      0.0          if action == env.action_space.n-1:
    33        44        199.8      4.5      0.0              out_state = state.copy()
    34        44         36.2      0.8      0.0              out_state[-1] = 1
    35        44         18.1      0.4      0.0              return True, out_state
    36                                                   else:
    37     27984      12553.0      0.4      0.0              return False, terminal_state
    38    608972     373800.3      0.6      0.5      action_valid = True
    39    608972     688477.2      1.1      0.9      num_tensor_actions = env.num_tensor_types * env.max_tensors
    40    608972    6633785.2     10.9      8.3      num_active_tensors = (state == 0).argmax(axis=0) # locates first 0 in the tensor arena
    41    608972     262589.2      0.4      0.3      num_legs_per_tensor = 5 # this will change later based on the environmnet
    42    608972     504529.0      0.8      0.6      num_active_legs = num_active_tensors  * num_legs_per_tensor
    43    608972     462409.7      0.8      0.6      if action < num_tensor_actions:
    44                                                   # double check that tensor is not already selected
    45                                                   # double check that previous tensor spots are already filled
    46      5736       8622.0      1.5      0.0          tensor_idx, tensor_type = action // env.num_tensor_types, action % env.num_tensor_types
    47      5736      11987.3      2.1      0.0          if state[tensor_idx] != 0 or (tensor_idx != 0 and state[tensor_idx -1] == 0):
    48      4780       2221.5      0.5      0.0              action_valid = False
    49      4780       5810.8      1.2      0.0              out_state = terminal_state
    50                                                   else:
    51       956       4356.7      4.6      0.0              out_state = state.copy()
    52       956       1886.7      2.0      0.0              out_state[tensor_idx] = 1 + tensor_type
    53    603236     622009.9      1.0      0.8      elif action < num_tensor_actions + env.num_leg_combinations:
    54                                                   # leg contractions
    55                                                   # check that there are no collisions
    56    602280    8294784.4     13.8     10.4          (leg1, leg2), possible_conflicted_contractions = env.get_leg_indices_from_contraction_index(action - num_tensor_actions)
    57                                                   #print(len(possible_conflicted_contractions))
    58                                                   #todo for文がないように書き換え
    59                                                   # for contraction_idx in possible_conflicted_contractions:
    60                                                   #     if state[contraction_idx + env.max_tensors] != 0:
    61                                                   #         action_valid = False
    62                                                   #         out_state = terminal_state
    63                                                   #         return action_valid, out_state
    64    602280    3337868.9      5.5      4.2          conflict_indices = possible_conflicted_contractions + env.max_tensors
    65    602280   15939965.9     26.5     20.0          if np.any(state[conflict_indices] != 0):
    66    161658      93086.3      0.6      0.1              action_valid = False
    67    161658     145861.1      0.9      0.2              out_state = terminal_state
    68    161658      80607.8      0.5      0.1              return action_valid, out_state     
    69                                                   # check that the legs in question are spawned by tensors
    70    440622     450751.8      1.0      0.6          if leg1 >= num_active_legs or leg2 >= num_active_legs:
    71    414920     213749.6      0.5      0.3              action_valid = False
    72    414920     361873.8      0.9      0.5              out_state = terminal_state
    73    440622     828434.2      1.9      1.0          if leg1//env.tensor_size == leg2 //env.tensor_size:
    74     66392      38083.4      0.6      0.0              action_valid = False
    75     66392      39298.1      0.6      0.0              out_state = terminal_state
    76                                           
    77                                           
    78                                                   # check that it doesn't leave us with too few legs
    79    440622    7938058.5     18.0      9.9          num_contractions = np.sum(state[num_tensor_actions:])
    80    440622     604409.8      1.4      0.8          num_contracted_legs = 2 * num_contractions
    81    440622    1119975.5      2.5      1.4          if num_active_legs - num_contracted_legs - 2 <= env.min_legs:
    82    189485     119318.0      0.6      0.1              action_valid = False
    83    189485     136610.2      0.7      0.2              out_state = terminal_state
    84                                           
    85                                                   else:
    86    251137     919646.3      3.7      1.2              out_state = state.copy()
    87    251137     418312.7      1.7      0.5              out_state[action] = 1
    88                                               else:
    89                                                   # terminating
    90       956      16822.9     17.6      0.0          if np.sum(state) < num_min_actions:
    91                                                       action_valid = False
    92                                                       out_state = terminal_state
    93                                                   else:
    94       956       4273.8      4.5      0.0              out_state = state.copy()
    95       956       1410.7      1.5      0.0              out_state[-1] = 1
    96                                           
    97    447314     333144.7      0.7      0.4      return action_valid, out_state

